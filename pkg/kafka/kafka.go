package kafka

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	"github.com/IBM/sarama"
)

// KafkaConfig é…ç½®
type KafkaConfig struct {
	Brokers []string
	GroupID string
	Topics  []string
}

// RetryMessage é‡è¯•æ¶ˆæ¯ç»“æ„
type RetryMessage struct {
	Message     *sarama.ProducerMessage
	RetryCount  int
	LastAttempt time.Time
}

// Producer ç”Ÿäº§è€…
type Producer struct {
	asyncProducer sarama.AsyncProducer
	retryQueue    chan *RetryMessage
	maxRetries    int
	retryDelay    time.Duration
}

// Consumer æ¶ˆè´¹è€…
type Consumer struct {
	group   sarama.ConsumerGroup
	topics  []string
	ready   chan bool
	Handler ConsumerHandler
}

type ConsumerHandler interface {
	HandleMessage(msg *sarama.ConsumerMessage) error
}

// InitProducer åˆå§‹åŒ–ç”Ÿäº§è€…
func InitProducer(brokers []string) (*Producer, error) {
	config := sarama.NewConfig()
	config.Producer.Return.Successes = true
	config.Producer.Return.Errors = true
	config.Producer.Partitioner = sarama.NewHashPartitioner
	config.Producer.Retry.Max = 3
	config.Producer.Retry.Backoff = 100 * time.Millisecond

	producer, err := sarama.NewAsyncProducer(brokers, config)
	if err != nil {
		return nil, err
	}

	p := &Producer{
		asyncProducer: producer,
		retryQueue:    make(chan *RetryMessage, 1000), // é‡è¯•é˜Ÿåˆ—
		maxRetries:    5,                              // æœ€å¤§é‡è¯•æ¬¡æ•°
		retryDelay:    2 * time.Second,                // é‡è¯•å»¶è¿Ÿ
	}

	// å¯åŠ¨é”™è¯¯å¤„ç†å’Œé‡è¯•goroutine
	go p.handleErrors()

	// å¯åŠ¨æˆåŠŸç›‘å¬goroutine
	go p.handleSuccesses()

	// å¯åŠ¨é‡è¯•å¤„ç†goroutine
	go p.handleRetries()

	return p, nil
}

// handleErrors å¤„ç†å‘é€é”™è¯¯ï¼Œå°†å¤±è´¥æ¶ˆæ¯åŠ å…¥é‡è¯•é˜Ÿåˆ—
func (p *Producer) handleErrors() {
	for err := range p.asyncProducer.Errors() {
		fmt.Printf("âŒ Kafka Produceré”™è¯¯: %v, topic=%s, partition=%d\n",
			err.Err, err.Msg.Topic, err.Msg.Partition)

		// åˆ›å»ºé‡è¯•æ¶ˆæ¯
		retryMsg := &RetryMessage{
			Message:     err.Msg,
			RetryCount:  0,
			LastAttempt: time.Now(),
		}

		// åŠ å…¥é‡è¯•é˜Ÿåˆ—
		select {
		case p.retryQueue <- retryMsg:
			fmt.Printf("ğŸ“ æ¶ˆæ¯å·²åŠ å…¥é‡è¯•é˜Ÿåˆ—: topic=%s\n", err.Msg.Topic)
		default:
			fmt.Printf("âš ï¸  é‡è¯•é˜Ÿåˆ—å·²æ»¡ï¼Œæ¶ˆæ¯ä¸¢å¼ƒ: topic=%s\n", err.Msg.Topic)
		}
	}
}

// handleSuccesses å¤„ç†å‘é€æˆåŠŸ
func (p *Producer) handleSuccesses() {
	for success := range p.asyncProducer.Successes() {
		fmt.Printf("âœ… Kafkaæ¶ˆæ¯å‘é€æˆåŠŸ: topic=%s, partition=%d, offset=%d\n",
			success.Topic, success.Partition, success.Offset)
	}
}

// handleRetries å¤„ç†é‡è¯•é˜Ÿåˆ—
func (p *Producer) handleRetries() {
	for retryMsg := range p.retryQueue {
		// æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
		if retryMsg.RetryCount >= p.maxRetries {
			fmt.Printf("âŒ æ¶ˆæ¯é‡è¯•æ¬¡æ•°è¶…é™ï¼Œæœ€ç»ˆä¸¢å¼ƒ: topic=%s, retries=%d\n",
				retryMsg.Message.Topic, retryMsg.RetryCount)
			continue
		}

		// ç­‰å¾…é‡è¯•å»¶è¿Ÿ
		time.Sleep(p.retryDelay)

		// å¢åŠ é‡è¯•æ¬¡æ•°
		retryMsg.RetryCount++
		retryMsg.LastAttempt = time.Now()

		fmt.Printf("ğŸ”„ é‡è¯•å‘é€æ¶ˆæ¯: topic=%s, attempt=%d/%d\n",
			retryMsg.Message.Topic, retryMsg.RetryCount, p.maxRetries)

		// é‡æ–°å‘é€æ¶ˆæ¯
		p.asyncProducer.Input() <- retryMsg.Message
	}
}

// SendMessage å‘é€æ¶ˆæ¯
func (p *Producer) SendMessage(topic string, key, value []byte) error {
	msg := &sarama.ProducerMessage{
		Topic: topic,
		Key:   sarama.ByteEncoder(key),
		Value: sarama.ByteEncoder(value),
	}

	fmt.Printf("ğŸ“¤ å‡†å¤‡å‘é€æ¶ˆæ¯åˆ°topic: %s, æ¶ˆæ¯å¤§å°: %d bytes\n", topic, len(value))

	// å‘é€æ¶ˆæ¯åˆ°å¼‚æ­¥é˜Ÿåˆ—
	p.asyncProducer.Input() <- msg
	fmt.Printf("ğŸ“¨ æ¶ˆæ¯å·²æäº¤åˆ°å¼‚æ­¥é˜Ÿåˆ—\n")

	return nil
}

// PublishMessage å‘é€JSONæ¶ˆæ¯
func (p *Producer) PublishMessage(topic string, data interface{}) error {
	jsonData, err := json.Marshal(data)
	if err != nil {
		return fmt.Errorf("JSONåºåˆ—åŒ–å¤±è´¥: %v", err)
	}

	return p.SendMessage(topic, nil, jsonData)
}

// Close å…³é—­ç”Ÿäº§è€…
func (p *Producer) Close() error {
	// å…³é—­é‡è¯•é˜Ÿåˆ—
	close(p.retryQueue)

	// å…³é—­å¼‚æ­¥ç”Ÿäº§è€…
	return p.asyncProducer.Close()
}

// GetRetryQueueSize è·å–é‡è¯•é˜Ÿåˆ—å¤§å°ï¼ˆç”¨äºç›‘æ§ï¼‰
func (p *Producer) GetRetryQueueSize() int {
	return len(p.retryQueue)
}

// InitConsumer åˆå§‹åŒ–æ¶ˆè´¹è€…
func InitConsumer(cfg KafkaConfig, handler ConsumerHandler) (*Consumer, error) {
	config := sarama.NewConfig()
	config.Consumer.Offsets.Initial = sarama.OffsetNewest
	group, err := sarama.NewConsumerGroup(cfg.Brokers, cfg.GroupID, config)
	if err != nil {
		return nil, err
	}
	c := &Consumer{
		group:   group,
		topics:  cfg.Topics,
		ready:   make(chan bool),
		Handler: handler,
	}
	return c, nil
}

// StartConsuming å¯åŠ¨æ¶ˆè´¹
func (c *Consumer) StartConsuming(ctx context.Context) error {
	go func() {
		for {
			fmt.Printf("ğŸ”„ æ¶ˆè´¹è€…å¼€å§‹æ–°çš„æ¶ˆè´¹å¾ªç¯...\n")
			if err := c.group.Consume(ctx, c.topics, c); err != nil {
				fmt.Printf("âŒ æ¶ˆè´¹è€…é”™è¯¯: %v\n", err)
			}
			if ctx.Err() != nil {
				fmt.Printf("ğŸ›‘ æ¶ˆè´¹è€…ä¸Šä¸‹æ–‡å–æ¶ˆï¼Œé€€å‡º\n")
				return
			}
			fmt.Printf("âš ï¸  æ¶ˆè´¹è€…å¾ªç¯ç»“æŸï¼Œé‡æ–°å¼€å§‹...\n")
		}
	}()
	<-c.ready
	fmt.Printf("âœ… æ¶ˆè´¹è€…å·²å‡†å¤‡å°±ç»ª\n")
	return nil
}

// Setup sarama.ConsumerGroupHandler
func (c *Consumer) Setup(_ sarama.ConsumerGroupSession) error {
	close(c.ready)
	return nil
}

// Cleanup sarama.ConsumerGroupHandler
func (c *Consumer) Cleanup(_ sarama.ConsumerGroupSession) error {
	return nil
}

// ConsumeClaim æ¶ˆè´¹æ¶ˆæ¯
func (c *Consumer) ConsumeClaim(sess sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	fmt.Printf("ğŸ¯ å¼€å§‹æ¶ˆè´¹åˆ†åŒº %d çš„æ¶ˆæ¯\n", claim.Partition())

	for msg := range claim.Messages() {
		fmt.Printf("ğŸ“¨ æ”¶åˆ°æ¶ˆæ¯: partition=%d, offset=%d\n", msg.Partition, msg.Offset)

		if err := c.Handler.HandleMessage(msg); err == nil {
			sess.MarkMessage(msg, "")
			fmt.Printf("âœ… æ¶ˆæ¯å·²æ ‡è®°ä¸ºå·²å¤„ç†: offset=%d\n", msg.Offset)
		} else {
			fmt.Printf("âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: %v, offset=%d\n", err, msg.Offset)
		}
	}

	fmt.Printf("ğŸ”š åˆ†åŒº %d çš„æ¶ˆæ¯æ¶ˆè´¹ç»“æŸ\n", claim.Partition())
	return nil
}
